# Introduction to NLP
More than 80% of the data being generated is in an unstructured format, maybe in the form of text, image, audio etc...

The majority of the data exists in the textual form. Use of Text Analysis can help extract insights from unstructured text.

Text data is most common and covers more than 50% of the unstructured data. Unstructure data is the information that doesn't reside in a tradational DB. Example includes: documents, blogs, social media feeds, pictures, and videos. Text data is most common and covers more than 50% of the unstructured data.

The ability to make machines understand and interpret the human language (text data)is termed as Natural Language Processing.

NLTK: Natural language toolkit and commonly called the mother of all NLP libraries. It is one of the mature primary resources when it comes to Python and NLP.

SpaCy: SpaCy is recently a trending library, as it comes with the added.flavors of a deep learning framework. While SpaCy doesn’t cover all of the NLP functionalities, the things that it does do, it does really well.

!pip install nltk

import nltk
nltk.download()

!pip install spacy

TextBlob: This is one of the data scientist’s favorite library when it comes to implementing NLP tasks.

!pip install textblob

CoreNLP: It is a Python wrapper for Stanford CoreNLP. The toolkit provides very robust, accurate, and optimized techniques for tagging, parsing, and analyzing text in various languages.

!pip install CoreNLP

## Free source A huge amount of data is freely available over the internet.

Free APIs like Twitter
Wikipedia
Government data (e.g. http://data.gov)
Census data (e.g. http://www.census.gov/data.html)
Health care claim data (e.g. https://www.healthdata.gov/)

## Chapter One: Extracting the Data
### Recipe 1. Text data collection using APIs
### Recipe 2. Reading PDF file in Python
### Recipe 3. Reading word document
### Recipe 4. Reading JSON object
### Recipe 5. Reading HTML page and HTML parsing
### Recipe 6. Regular expressions
### Recipe 7. String handling
### Recipe 8. Web scraping

### Recipe 1: Text Data Collection using Twitter APIs
There are a lot of free APIs through which we can collect data and use it to solve problems. The Twitter API in particular. Twitter has a gigantic amount of data with a lot of value in it. Social media marketers are making their living from it. There is an enormous amount of tweets every day, and every tweet has some story to tell. When all of this data is collected and analyzed, it gives a tremendous amount of insights to a business about their company, product, service, etc.

Create your own app in the Twitter developer portal, and get the keys mentioned below. Once you have these credentials, you can start pulling data.

consumer key
consumer secret
access token
access token secret
Once all the credentials are in place, use the code below to fetch the data.

# Install tweepy
!pip install tweepy

#Import the libraries 
import numpy as np
import tweepy
import json
import pandas as pd
from tweepy import OAuthHandler

# Credentials 
#consumer_key = ""
#consumer_secret = ""
#access_token = ""
#access_token_secret = ""

Bearer_Token="" #you only need this one for this implementation

# Calling API
#auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
#auth.set_access_token(access_token, access_token_secret)
#api = tweepy.API(auth)

# Calling API
auth = tweepy.OAuth2BearerHandler(Bearer_Token)
#auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth)

# Provide the query you want to pull the data. 
# For example, pulling data for the mobile phone ABC
query ="elonmusk"

# Fetching tweets
Tweets = api.search_tweets(query, count = 10,lang='en',tweet_mode='extended')
print(Tweets)


#Tweets = api.search_tweets(query, count = 1,lang='en', tweet_mode='extended')
#print(Tweets)

#The query above will pull the top 10 tweets when the product ABC is searched. 
#The API will pull English tweets since the language given is ‘en’ and it will exclude retweets.

## Recipe 2: Reading PDF file in Python
Most of the time your data will be stored as PDF files. We need to extract text from these files and store it for further analysis. The simplest way to do this is by using the PyPDF2 library.

#Install the necessary libraries
!pip install PyPDF2

#Import all the necessary libraries
import PyPDF2
from PyPDF2 import PdfFileReader

#Creating a pdf file object
pdf = open("samplepaper.pdf","rb")

#Creating pdf reader object
pdf_reader = PyPDF2.PdfFileReader(pdf)

#Checking number of pages in a pdf file
print("The total number of pages in the PDF file: ", pdf_reader.numPages)

#Creating a page object
page = pdf_reader.getPage(0)

#Finally extracting text from the page
print("The content of the first page is: ", page.extractText())

#Closing the pdf file
pdf.close()

#Please note that the function above doesn’t work for scanned PDFs.


## Recipe 3: Reading Word Document

#Install the docx library
!pip install docx

#Import library
from docx import Document
from docx import *  
import docx

#Creating a word file object
doc = open("CryptoReport.docx","rb")

#Creating word reader object
document = docx.Document(doc)

#Create an empty string and call this document.
docu=""

#Create a for loop that goes through each paragraph in the Word
#document and appends the paragraph.

for para in document.paragraphs:
    docu += para.text

#To see the output call docu
print(docu)

## Recipe 4: Reading JSON object

#importing the libraries
import json

json_file ='devinfo.json'
with open('devinfo.json','r') as f:
    data = json.load(f)

data

#To convert the json format to pandas (DataFrame)
import pandas as pd
df = pd.DataFrame(data)
df

## Recipe 5: Reading HTML page and HTML parsing

#install the bs4 library
!pip install bs4

#import the libraries
import urllib.request as urllib2
from bs4 import BeautifulSoup

#Fetch the HTML file
response = urllib2.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')
html_doc = response.read()

#Parse the HTML file
soup = BeautifulSoup(html_doc, 'html.parser')

# Formating the parsed html file
strhtm = soup.prettify()

# Print few lines
print (strhtm[:500])

#We can extract a tag value from the first instance of the tag using the following code.
print(soup.title)
print(soup.title.string)
print(soup.a.string)
print(soup.b.string)

#Here we get all the instances of a tag that we are interested in:
for x in soup.find_all('a'): print(x.string)

#Extracting all text of a particular tag
for x in soup.find_all('p'): print(x.text)

## Recipe 6: Regular expressions

We can do all sort of basic and advanced data cleaning using regular expressions. The best way to do this is by using the “re” library in Python.

#Regex: [ab] = find the single occurrence of character a and b #Regex: [^ab] = find characters except for a and b #Regex: [a-z] = find the character range of a to z #Regex: [^a-z] = find a range except to z #Regex: [a-zA-Z] = find all the characters a to z as well as A to Z #Regex: = any single character #Regex: \s = any whitespace character #Regex: \S = any non-whitespace character #Regex: \d = any digit #Regex: \D = any non-digit #Regex: \w = any words #Regex: \W = any non-words #Regex: (a|b) = either match a or b #Regex: a{3} = Exactly match three occurences of a #Regex: a{3,} = Match simultaneous occurrences of a with 3 or more than 3 #Regex: a{3,6} = Match simultaneous occurrences of a between 3 to 6 #Regex: ^ = Starting of the string #Regex: $ = Ending of the string #Regex: \b = Match word boundary #Regex: \B = Non-word boundary #wild card (?, *, +) ##### ? : matches zero or one occurrence but not more than one occurrence ##### * : matches zero or more than that occurrences ##### + : matches one or more than that occurrences# The diffrence between re.match() and re.search(): #######re.match(): this checks for a match of the string only at the beginning of the string. ######re.search(): this checks for a match of the string anywhere in the string.

### Tokenizing using re.split

# Import library
import re

#run the split query
re.split('\s+','I like this book.')

### Extracing email IDs using regular expression

# Import library
import re

#Example document 
doc = "For more details please mail us at: xyz@abc.com,pqr@mno.com"

#Execute the re.findall function
addresses = re.findall(r'[\w\.-]+@[\w\.-]+', doc)

for address in addresses:
    print(address)

### Replacing email IDs using re.sub

# Import library
import re

#Example document 
doc = "For more details please mail us at: xyz@abc.com"

#Execute the re.sub function
new_email_address = re.sub(r'([\w\.-]+)@([\w\.-]+)', r'pqr@mno.com', doc)

print(new_email_address)

### Extract data from the ebook and perform regex

# Import library
import re
import requests

#url to extract
url = 'https://www.gutenberg.org/files/2638/2638-0.txt'
raw = requests.get(url).text

#Defining the function to extract the info
"""def get_book(url):
    # Sends a http request to get the text from project Gutenberg
    raw = requests.get(url).text
    
    # Discards the metadata from the beginning of the book
    start = re.search(r"\*\*\* START OF THIS PROJECT GUTENBERG EBOOK .* \*\*\*", raw).end()
    
    # Discards the metadata from the end of the book
    stop = re.search(r"II", raw).start()
    
    # Keeps the relevant text
    text = raw[start:stop]
    
    return text"""
    
#Processing the text
def preprocess(sentence):
    return re.sub('[^A-Za-z0-9.]+' , ' ', sentence).lower()

#Calling the above two functions
#Book = get_book(url)
Processed_Book = preprocess(raw)

#Show the out put
print(Processed_Book[:500])

#Perform some exploratory data analysis on this data using regex
#Count number of times "the" is appeared in the book

len(re.findall(r'the', Processed_Book))

#Replace "i" with "I"
Processed_Book = re.sub(r'\si\s', " I ", Processed_Book)
print(Processed_Book[:500])

## Recipe 7: Handling Strings

# We can do all sort of basic text explorations using string operations. The simplest way to do this is by using the below string functionality.

s.find(t) index of first instance of string t inside s (-1 if not found)
s.rfind(t) index of last instance of string t inside s (-1 if not found)
s.index(t) like s.find(t) except it raises ValueError if not found
s.rindex(t) like s.rfind(t) except it raises ValueError if not found
s.join(text) combine the words of the text into a string using s as the glue
s.split(t) split s into a list wherever a t is found (whitespace by default)
s.splitlines() split s into a list of strings, one per line
s.lower() a lowercased version of the string s
s.upper() an uppercased version of the string s
s.title() a titlecased version of the string s
s.strip() a copy of s without leading or trailing whitespace
s.replace(t, u) replace instances of t with u inside s

### Replacing content

String = "I am exploring NLP"

#To extract particular character or range of characters from string
print(String[0])

#To extract the word exploring
print(String[5:14])

#Replace “exploring” with “learning” in the above string
String_2 = String.replace("exploring", "learning")
print(String_2)

#Concatenating two strings
s1 = "nlp"
s2 = "machine learning"
s3 = s1 + " " + s2
print(s3)

#Searching for a substring in a string
var="I am learning NLP"
f= "learn"
var.find(f)#index of first instance of string "learn"

## Recipe 8: Scraping Text from the Web
Caution Before scraping any websites, blogs, or e-commerce websites, please make sure you read the terms and conditions of the websites on whether it gives permissions for data scraping.

Web scraping, also called web harvesting or web data extraction. It is a technique to extract a large amount of data from websites and save it in a database or locally. You can use this data to extract information related to your customers/users/products for the business’s benefit.

#Install all the necessary libraries
!pip install bs4
!pip install requests

page = requests.get("https://dataquestio.github.io/web-scraping-pages/simple.html")
page

page.status_code

page.content

from bs4 import BeautifulSoup
soup = BeautifulSoup(page.content, 'html.parser')

print(soup.prettify())

list(soup.children)

[type(item) for item in list(soup.children)]

html = list(soup.children)[2]

list(html.children)

