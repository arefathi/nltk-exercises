{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb109af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word and Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad4d8f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c471a77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great and Python is awesome.', 'The sky is pinkish-blue.', 'You should not ept cardboard.']\n"
     ]
    }
   ],
   "source": [
    "#Word and sentence tokenization library \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#Text for tokenization\n",
    "text = \"\"\"Hello Mr. Smith, how are you doing today?\n",
    "                The weather is great and Python is awesome.\n",
    "                The sky is pinkish-blue. You should not ept cardboard.\"\"\"\n",
    "\n",
    "#Print the Sentence level toknization\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01607e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Mr. Smith, how are you doing today?\n",
      "The weather is great and Python is awesome.\n",
      "The sky is pinkish-blue.\n",
      "You should not ept cardboard.\n"
     ]
    }
   ],
   "source": [
    "#To print each sentence in a new line\n",
    "for sent in sent_tokenize(text):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a8c0dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "843ec7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', 'not', 'ept', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "#Print the Word level toknization\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a934a2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Mr.\n",
      "Smith\n",
      ",\n",
      "how\n",
      "are\n",
      "you\n",
      "doing\n",
      "today\n",
      "?\n",
      "The\n",
      "weather\n",
      "is\n",
      "great\n",
      "and\n",
      "Python\n",
      "is\n",
      "awesome\n",
      ".\n",
      "The\n",
      "sky\n",
      "is\n",
      "pinkish-blue\n",
      ".\n",
      "You\n",
      "should\n",
      "not\n",
      "ept\n",
      "cardboard\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#To print each Word in a new line\n",
    "for word in word_tokenize(text):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4e6ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ccca74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'too', 'them', 'or', 'hadn', 'now', 'had', 'yours', 'herself', \"should've\", 'i', 'her', 'between', \"you're\", 'doesn', 'd', 'because', 'were', 'nor', 'there', 've', 'against', \"isn't\", 'me', \"that'll\", 'which', 'these', 'has', \"hadn't\", 'can', 'm', 'aren', 'below', 'that', 'y', 'yourself', 'doing', 'each', 'weren', 'if', 'himself', 'by', \"needn't\", \"don't\", 'you', 'up', 'myself', 'yourselves', \"it's\", 'of', 'few', 'our', 'after', 'does', 'needn', 'ourselves', 'same', 'did', 'couldn', 'why', 'some', 'she', 'he', \"you'd\", 'while', \"doesn't\", 'for', 'both', 'hasn', 'as', 'before', 'hers', 'those', 'it', 'ain', 'how', \"haven't\", 'what', 'through', 'who', 'having', 'no', \"won't\", 'a', 'than', 'didn', 'very', 't', 'any', 'into', 'was', 'such', 'have', 'their', \"wouldn't\", \"mightn't\", 'isn', 'his', \"weren't\", 'here', 'more', 'this', 're', 'they', 'then', \"shouldn't\", 'your', 'itself', 'is', 'are', 'again', 's', \"couldn't\", 'about', 'will', 'and', 'over', 'so', 'do', \"aren't\", 'wasn', \"you'll\", \"mustn't\", 'shouldn', 'won', \"wasn't\", 'once', 'o', 'being', 'with', 'haven', 'where', \"she's\", 'out', 'when', 'an', 'be', 'wouldn', 'only', 'the', 'we', 'all', \"you've\", \"didn't\", 'down', 'most', 'whom', 'but', 'above', 'its', 'just', 'further', 'own', 'don', 'll', 'at', 'under', 'until', 'theirs', 'themselves', 'in', 'not', 'from', 'my', \"shan't\", 'shan', 'ours', 'ma', 'been', 'should', 'him', 'other', 'on', 'off', 'to', 'mustn', \"hasn't\", 'am', 'mightn', 'during'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords #Stop word library \n",
    "from nltk.tokenize import word_tokenize #Word tokenization library \n",
    "\n",
    "#Text for tokenization\n",
    "text = \"\"\"This is an example showing off stop word filteration.\"\"\"\n",
    "\n",
    "#English languge stop words  \n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2498139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1cbb62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words before removal of stop words:\n",
      "  ['This', 'is', 'an', 'example', 'showing', 'off', 'stop', 'word', 'filteration', '.']\n",
      "=====================================================================\n",
      "List of words after removal of stop words: \n",
      "  ['This', 'example', 'showing', 'stop', 'word', 'filteration', '.']\n"
     ]
    }
   ],
   "source": [
    "#Removing stop words from the given text \n",
    "words = word_tokenize(text)\n",
    "\n",
    "words_without_stopword = []\n",
    "\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        words_without_stopword.append(word)\n",
    "print(\"List of words before removal of stop words:\\n \", words) \n",
    "print(\"=====================================================================\")\n",
    "print(\"List of words after removal of stop words: \\n \", words_without_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad1e8e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words before stop word removal is:  10\n",
      "Number of words after stop word removal is:  7\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words before stop word removal is: \", len(words))\n",
    "print(\"Number of words after stop word removal is: \", len(words_without_stopword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a25f4b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'showing', 'stop', 'word', 'filteration', '.']\n"
     ]
    }
   ],
   "source": [
    "#Stop words removal in different way\n",
    "sentence_without_stopword = [word for word in words if not word in stop_words]\n",
    "print(sentence_without_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c19cf02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after stop word removal is:  7\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words after stop word removal is: \", len(sentence_without_stopword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "204ef4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Stemmer to stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "909aea21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer #Porter Stemmer  \n",
    "from nltk.tokenize import word_tokenize #Word tokenization library \n",
    "\n",
    "ps = PorterStemmer() #Porter Stemmer object  \n",
    "\n",
    "#List of words to stem\n",
    "words = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]\n",
    "\n",
    "for word in words:\n",
    "    print(ps.stem(word))#applying the stem object on the list of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f0952df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "is\n",
      "veri\n",
      "import\n",
      "to\n",
      "be\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onec\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"It is very important to be pythonly while you are pythoning with python.\n",
    "           All pythoners have pythoned poorly at least onec.\"\"\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "for word in words:\n",
    "    print(ps.stem(word))#applying the stem object on the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df4bfff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "783588b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\n",
      " \n",
      "February 2, 2005\n",
      "\n",
      "\n",
      "9:10 P.M. EST \n",
      "\n",
      "THE PRESIDENT: Mr. Speaker, Vice President Cheney, members of Congress, fellow citizens: \n",
      "\n",
      "As a new Congress gathers, all of us in the elected branches of government share a great privilege: We've been placed in office by the votes of the people we serve. And tonight that is a privilege we share with newly-elected leaders of Afghanistan, the Palestinian Territo\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer #Special type of tokenizer \n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "#To print GWBush 2005 speech\n",
    "print(train_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14f16939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\n",
      " \n",
      "January 31, 2006\n",
      "\n",
      "THE PRESIDENT: Thank you all. Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream. Tonight we are comforted by the hope of a glad reunion with the hus\n"
     ]
    }
   ],
   "source": [
    "#To print GWBush 2006 speech\n",
    "print(sample_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e689d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\n",
      " \n",
      "February 2, 2005\n",
      "\n",
      "\n",
      "9:10 P.M. EST \n",
      "\n",
      "THE PRESIDENT: Mr. Speaker, Vice President Cheney, members of Congress, fellow citizens: \n",
      "\n",
      "As a new Congress gathers, all of us in the elected branches of government share a great privilege: We've been placed in office by the votes of the people we serve. And tonight that is a privilege we share with newly-elected leaders of Afghanistan, the Palestinian Territo\n"
     ]
    }
   ],
   "source": [
    "train_text_short = train_text[:500]#Short form of the train text\n",
    "print(train_text_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c55eec14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\n",
      " \n",
      "January 31, 2006\n",
      "\n",
      "THE PRESIDENT: Thank you all. Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream. Tonight we are comforted by the hope of a glad reunion with the hus\n"
     ]
    }
   ],
   "source": [
    "sample_text_short = sample_text[:500]#Short form of the sample text\n",
    "print(sample_text_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0038c675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('hus', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text_short)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text_short)\n",
    "\n",
    "#Part of speech tagging\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d838ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part of Speech Tagging using Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf36ebb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "  'S/POS\n",
      "  (Chunk ADDRESS/NNP)\n",
      "  BEFORE/IN\n",
      "  (Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP UNION/NNP January/NNP)\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  (Chunk THE/NNP PRESIDENT/NNP)\n",
      "  :/:\n",
      "  (Chunk Thank/NNP)\n",
      "  you/PRP\n",
      "  all/DT\n",
      "  ./.)\n",
      "(S\n",
      "  (Chunk Mr./NNP Speaker/NNP)\n",
      "  ,/,\n",
      "  (Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  (Chunk Congress/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chunk Supreme/NNP Court/NNP)\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:\n",
      "  Today/VB\n",
      "  our/PRP$\n",
      "  nation/NN\n",
      "  lost/VBD\n",
      "  a/DT\n",
      "  beloved/VBN\n",
      "  ,/,\n",
      "  graceful/JJ\n",
      "  ,/,\n",
      "  courageous/JJ\n",
      "  woman/NN\n",
      "  who/WP\n",
      "  (Chunk called/VBD America/NNP)\n",
      "  to/TO\n",
      "  its/PRP$\n",
      "  founding/NN\n",
      "  ideals/NNS\n",
      "  and/CC\n",
      "  carried/VBD\n",
      "  on/IN\n",
      "  a/DT\n",
      "  noble/JJ\n",
      "  dream/NN\n",
      "  ./.)\n",
      "(S\n",
      "  Tonight/NN\n",
      "  we/PRP\n",
      "  are/VBP\n",
      "  comforted/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  hope/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  glad/JJ\n",
      "  reunion/NN\n",
      "  with/IN\n",
      "  the/DT\n",
      "  hus/NN)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "train_text_short = train_text[:500]\n",
    "sample_text_short = sample_text[:500]\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text_short)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text_short)\n",
    "\n",
    "#Part of speech tagging using chunk\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            print(chunked)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "train_text_short = train_text[:500]\n",
    "sample_text_short = sample_text[:500]\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text_short)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text_short)\n",
    "\n",
    "#Part of speech tagging using chunk\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            chunked.draw()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part of Speech Tagging using Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f04d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "train_text_short = train_text[:500]\n",
    "sample_text_short = sample_text[:500]\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text_short)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text_short)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\n",
    "                        }<VB.?|IN|DT|TO>{\n",
    "                        \"\"\"\n",
    "                        \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            chunked.draw\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a36db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Name Entity\n",
    "####Name Entity Examples:\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "train_text_short = train_text[:500]\n",
    "sample_text_short = sample_text[:500]\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text_short)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text_short)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            nameEnt = nltk.ne_chunk(tagged, binary=\"True\")\n",
    "            \n",
    "            nameEnt.draw()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7370de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "668bbd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781aa689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"better\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "\n",
    "\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\", 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef3aff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "histori\n",
      "histor\n",
      "final\n",
      "final\n",
      "final\n"
     ]
    }
   ],
   "source": [
    "#Stemming Vs Lemmatizing \n",
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = [\"history\", \"historical\", \"finally\", \"final\", \"finalized\"]\n",
    "\n",
    "for word in words:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b1866a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history\n",
      "historical\n",
      "finally\n",
      "final\n",
      "finalized\n"
     ]
    }
   ],
   "source": [
    "#Stemming Vs Lemmatizing\n",
    "#Lemmatizating\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"history\", \"historical\", \"finally\", \"final\", \"finalized\"]\n",
    "\n",
    "for word in words:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44a44e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e0ab929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ShadowX9\\anaconda3\\lib\\site-packages\\nltk\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "print(nltk.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c24ab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%appdata%` not found.\n"
     ]
    }
   ],
   "source": [
    "#To go to the corpora location\n",
    "%appdata%\n",
    "#Go to \"nltk_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9351e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1:5 And God called the light Day, and the darkness he called Night.', 'And the evening and the morning were the first day.', '1:6 And God said, Let there be a firmament in the midst of the waters,\\nand let it divide the waters from the waters.', '1:7 And God made the firmament, and divided the waters which were\\nunder the firmament from the waters which were above the firmament:\\nand it was so.', '1:8 And God called the firmament Heaven.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sample = gutenberg.raw(\"bible-kjv.txt\")\n",
    "\n",
    "tok = sent_tokenize(sample)\n",
    "print(tok[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ed1873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40f69704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "syns = wordnet.synsets(\"program\")\n",
    "\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9cbeabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('plan.n.01')\n"
     ]
    }
   ],
   "source": [
    "print(syns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d06abf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('plan.n.01.plan')\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].lemmas()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de741593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].lemmas()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98f7c8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a series of steps to be carried out or goals to be accomplished\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "176976b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c33e3443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Synonyms:\n",
      " {'right', 'adept', 'goodness', 'just', 'estimable', 'ripe', 'in_force', 'soundly', 'sound', 'unspoiled', 'full', 'effective', 'well', 'expert', 'dear', 'unspoilt', 'thoroughly', 'upright', 'beneficial', 'dependable', 'serious', 'trade_good', 'respectable', 'good', 'secure', 'undecomposed', 'practiced', 'commodity', 'skilful', 'honest', 'in_effect', 'near', 'proficient', 'salutary', 'honorable', 'safe', 'skillful'}\n",
      "List of Antonyms:\n",
      " {'ill', 'bad', 'badness', 'evilness', 'evil'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "print(\"List of Synonyms:\\n\", set(synonyms))\n",
    "print(\"List of Antonyms:\\n\", set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db8383de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"boat.n.01\")\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65b5abed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6956521739130435\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"car.n.01\")\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "561475ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"cat.n.01\")\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "423f384d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"kitten.n.01\")\n",
    "w2 = wordnet.synset(\"cat.n.01\")\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d50da18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
